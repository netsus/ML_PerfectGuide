{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 토픽 모델링 개요\n",
    "토픽 모델링: 문서들에 잠재되어 있는 공통된 토픽(주제)들을 추출해 내는 기법.<br>\n",
    "* 특징: 문서들이 가지는 주요 토픽의 분포도와 개별 토픽이 어떤 의미인지를 단어들의 결합으로 제공.\n",
    "\n",
    "<img src=https://blog.kakaocdn.net/dn/bOUok2/btrFUM2Mu3c/qfTklvDLjRkXCOaqQXDDz1/img.png width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토픽 모델링의 이해\n",
    ": 단어들의 조합이 퍼센트와 함께 나온다 -> 사람이 단어들과 퍼센트를 보고 Food인지 Animals인지 해석을 해야한다.\n",
    "<br><img src=https://blog.kakaocdn.net/dn/Ecu3H/btrFVh26A0j/3QgbKva7DoY0vAYpxylNO0/img.png width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-28T07:57:31.338635Z",
     "start_time": "2022-06-28T07:57:31.326413Z"
    }
   },
   "source": [
    "### 토픽 모델링 알고리즘의 유형\n",
    "1. LSA와 NMF는 행렬 분해 기반(SVD) 토픽 모델링.\n",
    "2. pLSA와 LDA는 확률 기반의 토픽 모델링이다.\n",
    "**토픽 모델링은 모두 2개의 전제를 기반으로 한다** \n",
    "1. 개별 문서는 혼합된 여러 주제로 구성. \n",
    "2. 개별 주제는 여러 개의 단어로 구성. \n",
    "<br><img src=https://blog.kakaocdn.net/dn/lWzXm/btrFYwLKdEw/1GaLwjfowVLR2Ihc5aZUX1/img.png width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행렬 분해 기반, 확률 기반 시각화\n",
    "1. 행렬 분해 기반은 SVD(특이값 분해)기반\n",
    "2. 확률 기반은 토픽별 확률 분포\n",
    "<br><img src=https://blog.kakaocdn.net/dn/clIKr3/btrFYxYfdxh/PEpF8RO5JDPoamtH9qbiPk/img.png width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA의 이해\n",
    "데이터를 Document-Term 행렬로 변환 -> 베이즈 추론 -> 문서별 토픽 분포, 토픽별 단어 분포<br>\n",
    "즉, 문서내 단어들을 기반으로 베이즈 추론을 통해 \"문서내 토픽 분포\"와 \"토픽별 단어 분포\"를 추론하는 방식(사전 확률분포로 디리클레 분포 사용)\n",
    "<br><img src=https://blog.kakaocdn.net/dn/nclsX/btrFWbOvNKV/HKXmik4PxkbHak2goiUYu0/img.png width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 베이즈 추론 켤레 사전 분포\n",
    "베이지안 최적화에서 사후 확률을 계속 업데이트하며 최적의 파라미터를 찾는것과 비슷하다.<br>\n",
    "-> 관측되는 단어분포와 디리클레 사전 확률분포를 결합 -> 문서의 주제 분포와 주제의 단어 분포들의 사후 확률 분포를 업데이트하며 최적을 찾아가는 느낌.<br>\n",
    "<br>\n",
    "디리클레 분포를 사전 확률 분포로 사용 -> 다항 분포 예측(여러개 중에 하나가 발생하는 사건이 여러 번 반복되는 확률 분포. 예를 들어, 로또 100개 -> 1등~5등 몇번나왔는지 확률분포)\n",
    "<br><img src=https://blog.kakaocdn.net/dn/dLPk20/btrFWx5xrCk/10m0Ge5sA7JKIe0nZrbySK/img.png width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 구성 요소\n",
    "1. 세타(문서의 토픽 분포)는 초기값이 알파이고, 사후 확률 분포를 업데이트하며 최적을 찾아감.\n",
    "-> M(문서의 개수)번 만큼 반복<br>\n",
    "2. 에타(토픽의 단어 분포)는 초기값이 베타이고, 사후 확률 분포를 업데이트하며 최적을 찾아감.\n",
    "-> K(토픽의 개수)번 만큼 반복<br>\n",
    "**대강 LDA는 \"문서의 토픽 분포\", \"토픽의 단어 분포\"를 베이즈 추론으로 찾아내는 것이다. (이때 사용되는 사전확률 분포가 디리클레 분포다)**정도만 알아도 된다.\n",
    "<br><img src=https://blog.kakaocdn.net/dn/t78xO/btrFViHKpEu/l81CWpkMlVmhsh8hlciRJK/img.png width=1000>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 수행 프로세스\n",
    "**LDA는 반드시 단순 Count 기반으로 해야한다.(TF-IDF 안된다.)**<br>\n",
    "토픽 개수(K)설정 -> 알파, 베타 최초 할당 -> 모든 단어들의 토픽 할당 분포가 변경되지 않고 수렴될 때 까지 3번 4번 반복<br>\n",
    "<br><img src=https://blog.kakaocdn.net/dn/mKwAX/btrFZkYovmv/1bRMoxQB31xmEXp68fBImK/img.png width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA 단점\n",
    "- 추출된 토픽은 단어들의 결합만 퍼센트로 제공하기 때문에, 사람의 주관적인 해석이 필요하다.<br>\n",
    "- 초기화 파라미터(토픽 개수 K, 알파, 베타) 및 Document-Term 행렬의 단어 피처들을 어떻게 필터링할지 최적화가 어려움<br>\n",
    "\n",
    "**-> 제일 어려운게 토픽 개수 정하는 것. 어떻게 정하냐에 따라 전혀 안맞기도 한다**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사이킷런 LDA\n",
    "초기화 파라미터: 토픽의 개수, 알파, 베타, 최대 반복횟수 <br>\n",
    "<br><img src=https://blog.kakaocdn.net/dn/rONBJ/btrFZlbUGKz/FxyXiAOdHLDgS7QFZlO7Nk/img.png width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20 Newsgroup 토픽 모델링 실습\n",
    "\n",
    "**20개 중 8개의 주제 데이터 로드 및 Count기반 피처 벡터화. LDA는 Count기반 Vectorizer만 적용합니다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 모토사이클, 야구, 그래픽스, 윈도우즈, 중동, 기독교, 전자공학, 의학 등 8개 주제를 추출. \n",
    "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x',\n",
    "        'talk.politics.mideast', 'soc.religion.christian', 'sci.electronics', 'sci.med'  ]\n",
    "\n",
    "# 위에서 cats 변수로 기재된 category만 추출. featch_20newsgroups( )의 categories에 cats 입력\n",
    "news_df= fetch_20newsgroups(subset='all',remove=('headers', 'footers', 'quotes'), \n",
    "                            categories=cats, random_state=0)\n",
    "\n",
    "#LDA 는 Count기반의 Vectorizer만 적용합니다.  \n",
    "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english', ngram_range=(1,2))\n",
    "feat_vect = count_vect.fit_transform(news_df.data)\n",
    "print('CountVectorizer Shape:', feat_vect.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LDA 객체 생성 후 Count 피처 벡터화 객체로 LDA수행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=8, random_state=0)\n",
    "lda.fit(feat_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**각 토픽 모델링 주제별 단어들의 연관도 확인**  \n",
    "lda객체의 components_ 속성은 주제별로 개별 단어들의 연관도 정규화 숫자가 들어있음\n",
    "\n",
    "shape는 주제 개수 X 피처 단어 개수  \n",
    "\n",
    "components_ 에 들어 있는 숫자값은 각 주제별로 단어가 나타난 횟수를 정규화 하여 나타냄.   \n",
    "\n",
    "숫자가 클 수록 토픽에서 단어가 차지하는 비중이 높음  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda.components_.shape)\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**각 토픽별 중심 단어 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topic_words(model, feature_names, no_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print('\\nTopic #',topic_index)\n",
    "\n",
    "        # components_ array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array index를 반환. \n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        top_indexes=topic_word_indexes[:no_top_words]\n",
    "        \n",
    "        # top_indexes대상인 index별로 feature_names에 해당하는 word feature 추출 후 join으로 concat\n",
    "        feature_concat = ' + '.join([str(feature_names[i])+'*'+str(round(topic[i],1)) for i in top_indexes])                \n",
    "        print(feature_concat)\n",
    "\n",
    "# CountVectorizer객체내의 전체 word들의 명칭을 get_features_names( )를 통해 추출\n",
    "feature_names = count_vect.get_feature_names()\n",
    "\n",
    "# Topic별 가장 연관도가 높은 word를 15개만 추출\n",
    "display_topic_words(lda, feature_names, 15)\n",
    "\n",
    "# 모토사이클, 야구, 그래픽스, 윈도우즈, 중동, 기독교, 전자공학, 의학 등 8개 주제를 추출. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**개별 문서별 토픽 분포 확인**\n",
    "\n",
    "lda객체의 transform()을 수행하면 개별 문서별 토픽 분포를 반환함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics = lda.transform(feat_vect)\n",
    "print(doc_topics.shape)\n",
    "print(doc_topics[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**개별 문서별 토픽 분포도를 출력**\n",
    "\n",
    "20newsgroup으로 만들어진 문서명을 출력.\n",
    "\n",
    "fetch_20newsgroups()으로 만들어진 데이터의 filename속성은 모든 문서의 문서명을 가지고 있음.\n",
    "\n",
    "filename속성은 절대 디렉토리를 가지는 문서명을 가지고 있으므로 '\\\\'로 분할하여 맨 마지막 두번째 부터 파일명으로 가져옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename_list(newsdata):\n",
    "    filename_list=[]\n",
    "\n",
    "    for file in newsdata.filenames:\n",
    "            #print(file)\n",
    "            filename_temp = file.split('\\\\')[-2:]\n",
    "            filename = '.'.join(filename_temp)\n",
    "            filename_list.append(filename)\n",
    "    \n",
    "    return filename_list\n",
    "\n",
    "filename_list = get_filename_list(news_df)\n",
    "print(\"filename 개수:\",len(filename_list), \"filename list 10개만:\",filename_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DataFrame으로 생성하여 문서별 토픽 분포도 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "topic_names = ['Topic #'+ str(i) for i in range(0, 8)]\n",
    "doc_topic_df = pd.DataFrame(data=doc_topics, columns=topic_names, index=filename_list)\n",
    "doc_topic_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
